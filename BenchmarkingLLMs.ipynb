{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "INCMcH9JfzD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e51cf1-e4cb-4fe4-8dbd-36dfa9b3e31a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m546.2/546.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -qq install langchain_openai\n",
        "!pip -qq install PyPDF2\n",
        "!pip -qq install langchain_community\n",
        "!pip -qq install faiss-gpu-cu11\n",
        "!pip -qq install streamlit pyngrok\n",
        "!pip -qq install streamlit_modal\n",
        "!pip -qq install nltk\n",
        "!pip -qq install codebleu\n",
        "!pip -qq install rouge\n",
        "!pip -qq install mistralai\n",
        "!pip -qq install -U langchain-mistralai\n",
        "!pip -qq install plotly\n",
        "!pip -qq install langchain\n",
        "!pip -qq install langchain_huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\".env\", \"w\") as f:\n",
        "    from google.colab import userdata\n",
        "    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "    f.write(f\"GITHUB_TOKEN={GITHUB_TOKEN}\\n\")\n",
        "\n",
        "    huggingface_api_key = userdata.get('HF_TOKEN')\n",
        "    f.write(f\"HF_TOKEN={huggingface_api_key}\\n\")\n",
        "\n",
        "    gemma_key = userdata.get('GEMMA')\n",
        "    f.write(f'GEMMA={gemma_key}\\n')\n",
        "\n",
        "    ngrok_key = userdata.get('NGROK_KEY')\n",
        "    f.write(f'ngrok_token={ngrok_key}\\n')"
      ],
      "metadata": {
        "id": "kPVNCufc8ULh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile analyzer.py\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from rouge import Rouge\n",
        "from codebleu import calc_codebleu\n",
        "\n",
        "class EvaluationMetrics:\n",
        "    def __init__(self):\n",
        "        self.rouge = Rouge()\n",
        "        self.smoothing_function = SmoothingFunction().method1\n",
        "\n",
        "    def calculate_bleu_score(self, reference, hypothesis):\n",
        "        reference = [reference.split()]\n",
        "        hypothesis = hypothesis.split()\n",
        "        score = corpus_bleu([reference], [hypothesis], smoothing_function=self.smoothing_function)\n",
        "        return score\n",
        "\n",
        "    def calculate_rouge_scores(self, reference, hypothesis):\n",
        "        scores = self.rouge.get_scores(hypothesis, reference, avg=True)\n",
        "        return scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgqQdxhFme_N",
        "outputId": "68743f77-cae2-45bc-9583-f8a9605ee89b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting analyzer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile summarizer.py\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import os\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "from mistralai.client import MistralClient\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"GITHUB_TOKEN\")\n",
        "hf_key = os.getenv(\"HF_TOKEN\")\n",
        "gemma_key = os.getenv(\"GEMMA\")\n",
        "\n",
        "class Summarizer:\n",
        "    def __init__(self, document_file, summary_file):\n",
        "        self.document_file = document_file\n",
        "        self.summary_file = summary_file\n",
        "        self.document_text=\"\"\n",
        "        self.summary_text_golden =\"\"\n",
        "        self.summarised_text=\"\"\n",
        "\n",
        "    def get_pdf_text(self, pdf):\n",
        "        text = \"\"\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        for page in pdf_reader.pages:\n",
        "            text+= page.extract_text()\n",
        "        return text\n",
        "\n",
        "    def process_documents(self):\n",
        "        self.document_text = self.get_pdf_text(self.document_file) if self.document_file else \"\"\n",
        "        self.summary_text = self.get_pdf_text(self.summary_file) if self.summary_file else \"\"\n",
        "        return self.document_text, self.summary_text\n",
        "\n",
        "    def summarise_documents(self, model='gpt-4', task=\"Summarize\"):\n",
        "        # Modify the prompt based on the task\n",
        "        if task == \"Summarize\":\n",
        "            template = '''Generate a Resume Summary Script (6–8 sentences) for an engaging video presentation aimed at a hiring manager.\n",
        "            - Tone: Friendly, Confident, Assertive.\n",
        "            - Base the script strictly on the given resume content — do not hallucinate or add unverifiable details.:\\n {resume}'''\n",
        "\n",
        "        prompt = PromptTemplate(input_variables=['resume'],template=template)\n",
        "        output_parser = StrOutputParser()\n",
        "        model = self._get_model_interface(model)\n",
        "        chain = RunnableSequence(prompt, model, output_parser)\n",
        "\n",
        "        summarised_text = chain.invoke({'resume': self.document_text})\n",
        "\n",
        "        return summarised_text\n",
        "\n",
        "    def _get_model_interface(self, model_name):\n",
        "        if model_name == 'GPT-4o-mini':\n",
        "            return ChatOpenAI(\n",
        "                    base_url = \"https://models.github.ai/inference\",\n",
        "                    api_key= api_key,\n",
        "                    model=\"openai/gpt-4o-mini\",\n",
        "                    temperature=0.1\n",
        "                    )\n",
        "        if model_name == 'GPT-4.1-nano':\n",
        "            return ChatOpenAI(\n",
        "                    base_url = \"https://models.github.ai/inference\",\n",
        "                    api_key=api_key,\n",
        "                    model=\"openai/gpt-4.1-nano\",\n",
        "                    temperature=0.1\n",
        "                    )\n",
        "        if model_name == 'Mistral Small 3.1':\n",
        "            return ChatMistralAI(\n",
        "                    base_url = \"https://models.github.ai/inference\",\n",
        "                    api_key= api_key,\n",
        "                    model=\"mistral-small-2503\",\n",
        "                    temperature=0.1\n",
        "                    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JdOfGeRoxYU",
        "outputId": "a1a3de52-0531-4216-9bdd-adc6dafecd07"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting summarizer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile visualization.py\n",
        "import plotly.graph_objs as go\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "class ScoreVisualizer:\n",
        "    def __init__(self, all_scores, task):\n",
        "        self.all_scores = all_scores\n",
        "        self.task = task\n",
        "        self.color_map = {\n",
        "            'GPT-4o-mini': 'red',\n",
        "            'GPT-4.1-nano': 'purple',\n",
        "            'Mistral Small 3.1': 'blue',\n",
        "        }\n",
        "\n",
        "    def plot_scores(self):\n",
        "        data = []\n",
        "        BAR_WIDTH = 0.9 / (len(self.all_scores) + 1)\n",
        "\n",
        "        for model, scores in self.all_scores.items():\n",
        "            model_color = self.color_map.get(model, 'gray')\n",
        "            showlegend = True\n",
        "\n",
        "            if self.task == \"Summarize\":\n",
        "                bleu_trace = go.Bar(\n",
        "                    name=model,\n",
        "                    x=['BLEU'],\n",
        "                    y=[scores['bleu_score']],\n",
        "                    marker_color=model_color,\n",
        "                    width=BAR_WIDTH,\n",
        "                    legendgroup=model,\n",
        "                    showlegend=showlegend\n",
        "                )\n",
        "                data.append(bleu_trace)\n",
        "\n",
        "                showlegend = False\n",
        "\n",
        "                for rouge_key in ['rouge-1', 'rouge-2', 'rouge-l']:\n",
        "                    rouge_trace = go.Bar(\n",
        "                        name=model,\n",
        "                        x=[rouge_key.upper()],\n",
        "                        y=[scores['rouge_scores'][rouge_key]['f']],\n",
        "                        marker_color=model_color,\n",
        "                        width=BAR_WIDTH,\n",
        "                        legendgroup=model,\n",
        "                        showlegend=showlegend\n",
        "                    )\n",
        "                    data.append(rouge_trace)\n",
        "        layout = go.Layout(\n",
        "            title= 'Summary Evaluation Scores',\n",
        "            barmode='group',\n",
        "            yaxis=dict(title='Score'),\n",
        "            xaxis=dict(title='Metric'),\n",
        "            legend=dict(groupclick=\"toggleitem\")\n",
        "        )\n",
        "\n",
        "        fig = go.Figure(data=data, layout=layout)\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "    def render_score_table(self):\n",
        "        table_data = {}\n",
        "        for model, metrics in self.all_scores.items():\n",
        "            table_data[model] = {'Model': model}\n",
        "            if self.task == \"Summarize\":\n",
        "                for metric, score in metrics.items():\n",
        "                    if metric == 'rouge_scores':\n",
        "                        for rouge_metric, rouge_scores in score.items():\n",
        "                            table_data[model][f\"{rouge_metric.upper()} F1\"] = round(rouge_scores['f'], 3)\n",
        "                    else:\n",
        "                        metric_name = \"BLEU\"\n",
        "                        table_data[model][metric_name] = round(score, 3)\n",
        "\n",
        "        score_df = pd.DataFrame.from_dict(table_data, orient='index').reset_index(drop=True)\n",
        "\n",
        "        def highlight_max(s, props=''):\n",
        "            return np.where(s == np.nanmax(s.to_numpy()), props, '')\n",
        "\n",
        "        styled_df = score_df.style.apply(highlight_max, props='background-color:yellow;', axis=0,\n",
        "                                         subset=pd.IndexSlice[:, :] if self.task == \"Summarize\" else pd.IndexSlice[:, 'Codebleu':'Dataflow Match Score'])\n",
        "        st.dataframe(styled_df, use_container_width=True, hide_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrIIW_fCrhdI",
        "outputId": "ee2b6f5e-5c76-4f31-c8c3-08a2cd489a4b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting visualization.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from summarizer import Summarizer\n",
        "import os\n",
        "from analyzer import EvaluationMetrics\n",
        "from visualization import ScoreVisualizer\n",
        "\n",
        "class SummarizationApp:\n",
        "    def __init__(self):\n",
        "        self.model_options = ['GPT-4.1-nano', 'GPT-4o-mini', 'Mistral Small 3.1']\n",
        "\n",
        "    def render_sidebar(self):\n",
        "        st.sidebar.header(\"Upload Files\")\n",
        "        document_upload = st.sidebar.file_uploader(\"Upload Document\", type=['pdf'])\n",
        "        summary_upload = st.sidebar.file_uploader(\"Upload 'Golden Summary'\", type=['pdf'])\n",
        "\n",
        "        return document_upload, summary_upload\n",
        "\n",
        "    def select_models(self):\n",
        "        selected_models = st.sidebar.multiselect(\"Choose models to benchmark:\", self.model_options)\n",
        "        return selected_models\n",
        "\n",
        "    def execution(self):\n",
        "        run_button = st.sidebar.button(\"Run Benchmark\")\n",
        "        return run_button\n",
        "\n",
        "    def select_task(self):\n",
        "        task = st.sidebar.radio(\"Choose Task:\", [\"Summarize\"])\n",
        "        return task\n",
        "\n",
        "    def run(self):\n",
        "        st.title(\"LLM Benchmarking Tool\")\n",
        "        st.subheader(\"Summarization\")\n",
        "\n",
        "        document_upload, summary_upload= self.render_sidebar()\n",
        "        selected_models = self.select_models()\n",
        "        task = self.select_task()\n",
        "        run_button = self.execution()\n",
        "\n",
        "        if run_button:\n",
        "            if document_upload is not None and summary_upload is not None:\n",
        "                st.success(\"Files uploaded successfully!\")\n",
        "                st.write(\"Selected models for benchmarking:\", ', '.join(selected_models))\n",
        "                summarizer = Summarizer(document_upload, summary_upload)\n",
        "                document_text, summary_text_golden = summarizer.process_documents()\n",
        "\n",
        "                if task == \"Summarize\":\n",
        "                    st.subheader(\"'Golden Source' Target Summary Text:\")\n",
        "                with st.expander(\"Click for 'Golden Source' text\"):\n",
        "                    st.write(summary_text_golden)\n",
        "                all_scores={}\n",
        "                for model in selected_models:\n",
        "                    summarizer = Summarizer(document_upload, summary_upload)\n",
        "                    document_text, summary_text_golden = summarizer.process_documents()\n",
        "\n",
        "                    generated_summary = summarizer.summarise_documents(model, task)\n",
        "                    st.subheader(model)\n",
        "                    with st.expander(f\"Click to see {model} generated summary\"):\n",
        "                        st.write(generated_summary)\n",
        "\n",
        "                    evaluation_metrics = EvaluationMetrics()\n",
        "                    if task == \"Summarize\":\n",
        "                        bleu_score = evaluation_metrics.calculate_bleu_score(summary_text_golden, generated_summary)\n",
        "                        rouge_scores = evaluation_metrics.calculate_rouge_scores(summary_text_golden, generated_summary)\n",
        "                        all_scores[model] = {\n",
        "                            \"bleu_score\": bleu_score,\n",
        "                            \"rouge_scores\": rouge_scores\n",
        "                        }\n",
        "                visualizer = ScoreVisualizer(all_scores, task)\n",
        "                visualizer.plot_scores()\n",
        "                visualizer.render_score_table()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app = SummarizationApp()\n",
        "    app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU-AUpaKlWzV",
        "outputId": "f9330601-8463-4571-de01-ea46791b0509"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "import os\n",
        "ngrok_token = os.getenv(\"ngrok_token\")\n",
        "!ngrok config add-authtoken {ngrok_token}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44-RbniZuVRf",
        "outputId": "3fad3841-6f6a-43d7-d710-c93f45e0d5f3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:dotenv.main:python-dotenv could not parse statement starting at line 4\n",
            "WARNING:dotenv.main:python-dotenv could not parse statement starting at line 5\n",
            "WARNING:dotenv.main:python-dotenv could not parse statement starting at line 7\n",
            "WARNING:dotenv.main:python-dotenv could not parse statement starting at line 10\n",
            "WARNING:dotenv.main:python-dotenv could not parse statement starting at line 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(addr=8501)\n",
        "print(f\"Streamlit URL: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ClJrYcBuM_S",
        "outputId": "37e51e14-62d4-4393-f45c-87309ed1a916"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit URL: NgrokTunnel: \"https://708d48077f66.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py&"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klFOFQP7ujxS",
        "outputId": "52fe8608-a6e1-42c1-a659-ab5e15f8525b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-08-20T05:38:06+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.148.230.72:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-08-20 05:38:30.764109: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755668310.797473    6351 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755668310.807416    6351 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755668310.836879    6351 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755668310.836928    6351 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755668310.836948    6351 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755668310.836952    6351 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-20 05:38:30.846309: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/summarizer.py:13: UserWarning: Importing HuggingFacePipeline from langchain root module is no longer supported. Please use langchain_community.llms.huggingface_pipeline.HuggingFacePipeline instead.\n",
            "  from langchain import HuggingFacePipeline\n",
            "WARNING:dotenv.main:python-dotenv could not parse statement starting at line 4\n",
            "WARNING:dotenv.main:python-dotenv could not parse statement starting at line 5\n",
            "WARNING:dotenv.main:python-dotenv could not parse statement starting at line 7\n",
            "WARNING:dotenv.main:python-dotenv could not parse statement starting at line 10\n",
            "WARNING:dotenv.main:python-dotenv could not parse statement starting at line 11\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kill streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f_jZ1d4uqK2",
        "outputId": "cca10e21-6347-46a9-86da-455eee89a13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: kill: streamlit: arguments must be process or job IDs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mZGhAfMW9c2S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}