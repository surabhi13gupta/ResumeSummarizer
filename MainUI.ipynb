{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surabhi13gupta/ResumeSummarizer/blob/main/MainUI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAOlAmyZD_xz",
        "outputId": "0b561700-8839-46db-ce35-79917336413d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -qq install langchain_openai\n",
        "!pip -qq install PyPDF2\n",
        "!pip -qq install langchain_community\n",
        "!pip -qq install faiss-gpu-cu11\n",
        "!pip -qq install streamlit pyngrok\n",
        "!pip -qq install streamlit_modal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0mjn-Ef16PG",
        "outputId": "9904062d-1d84-4594-ed6f-38ff88480015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF7CnnjJ32wb",
        "outputId": "4b743ae8-b5ec-4f3f-f736-218afc7beb33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -qq install PyMuPDF\n",
        "!pip -qq install python-docx\n",
        "!pip -qq install docx2pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mAg4hFoVEebB"
      },
      "outputs": [],
      "source": [
        "with open(\".env\", \"w\") as f:\n",
        "    from google.colab import userdata\n",
        "    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "    f.write(f\"GITHUB_TOKEN={GITHUB_TOKEN}\\n\")\n",
        "\n",
        "    ngrok_key = userdata.get('NGROK_KEY')\n",
        "    f.write(f'ngrok_token={ngrok_key}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8YmLhSHq1-83"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/CDS-B9-Group11/Capstone Project/Codes/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSFvzyeSU4qB",
        "outputId": "42b6811e-219b-458d-c8df-7aa547ea8feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "import os\n",
        "ngrok_token = os.getenv(\"ngrok_token\")\n",
        "!ngrok config add-authtoken {ngrok_token}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSZOhWyNDupb",
        "outputId": "a9a9edcd-1254-4103-fd33-05c0a1f05715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting resume_uploader.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile resume_uploader.py\n",
        "import zipfile\n",
        "import os\n",
        "import tempfile\n",
        "from PyPDF2 import PdfReader\n",
        "import docx\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/CDS-B9-Group11/Capstone Project/Codes/')\n",
        "from masker import redact_personal_information\n",
        "\n",
        "def extract_text_from_pdf(file):\n",
        "    reader = PdfReader(file)\n",
        "    return \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
        "\n",
        "def extract_text_from_docx(file):\n",
        "    doc = docx.Document(file)\n",
        "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "def extract_from_zip(zip_file):\n",
        "    extracted_texts = []\n",
        "    with zipfile.ZipFile(zip_file) as z:\n",
        "        for name in z.namelist():\n",
        "            ext = name.rsplit('.', 1)[-1].lower()\n",
        "            with z.open(name) as f:\n",
        "                if ext == \"pdf\":\n",
        "                    modified_pdf = redact_personal_information(f)\n",
        "                    extracted_texts.append(modified_pdf(f))\n",
        "                elif ext == \"docx\":\n",
        "                    extracted_texts.append(extract_text_from_docx(f))\n",
        "    return extracted_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H8Slv3u9ku_",
        "outputId": "b9e01999-9ff0-4dc9-b94d-26b35453c924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/CDS-B9-Group11/Capstone Project/Codes/')\n",
        "\n",
        "import streamlit as st\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from google.colab import userdata\n",
        "from datetime import datetime\n",
        "import os\n",
        "from streamlit_modal import Modal\n",
        "from io import BytesIO\n",
        "from resume_uploader import extract_text_from_pdf, extract_text_from_docx, extract_from_zip\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from masker import redact_personal_information\n",
        "import pandas as pd\n",
        "\n",
        "load_dotenv()\n",
        "github_token = os.getenv(\"GITHUB_TOKEN\")\n",
        "endpoint = \"https://models.github.ai/inference\"\n",
        "model_name = \"openai/gpt-4.1-nano\"\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 10000,\n",
        "    chunk_overlap = 100\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "def get_vector_store(text_chunks):\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model='text-embedding-3-small',\n",
        "        dimensions=1536,\n",
        "        base_url = endpoint,\n",
        "        api_key= github_token)\n",
        "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "    vector_store.save_local(\"faiss_index\")\n",
        "\n",
        "\n",
        "def get_conversation_chain():\n",
        "    model = ChatOpenAI(\n",
        "    base_url = endpoint,\n",
        "    api_key= github_token,\n",
        "    model=model_name,\n",
        "    temperature=0.1\n",
        "    )\n",
        "    chat_prompt_template = ChatPromptTemplate([\n",
        "         ('system', 'You are good resume parser expert and helps in answering questions. Answer the question from the provided resume. Make sure to provide all the details, if the answer is not in the provided resume just reply back, \"Answer is not available in provided resume\", dont provide the wrong answers or dont hallucinate the answers.\\n Resume: \\n {resume}?'),\n",
        "         ('human', 'Question: \\n {question}\\nAnswer:')\n",
        "        ])\n",
        "    chain = chat_prompt_template|model|StrOutputParser()\n",
        "    return chain\n",
        "\n",
        "def notice_period_prompt():\n",
        "    model = ChatOpenAI(\n",
        "    base_url = endpoint,\n",
        "    api_key= github_token,\n",
        "    model=model_name,\n",
        "    temperature=0.1\n",
        "    )\n",
        "    chat_prompt_template = ChatPromptTemplate([\n",
        "         ('system', 'You are good and helpful assistant and helps in answering questions. Find out the notice period from public available sources for most recent organization provided in resume. Make sure to provide to provide accurate details and provide the source link at the end of answer, dont provide the wrong answers or dont hallucinate the answers. \\n Resume: \\n {resume}?'),\n",
        "         ('human', 'Question: \\n Notice Period of the most recent job organization the candidate working for. \\nAnswer:')\n",
        "        ])\n",
        "    chain = chat_prompt_template|model|StrOutputParser()\n",
        "    return chain\n",
        "\n",
        "def handle_notice_period():\n",
        "    user_question = \"Notice Period of the most recent job organization the candidate working for\"\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model='text-embedding-3-small',\n",
        "        dimensions=1536,\n",
        "        base_url = endpoint,\n",
        "        api_key= github_token,)\n",
        "    new_db = FAISS.load_local(\"faiss_index\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
        "    docs = new_db.similarity_search(user_question)\n",
        "    chain = notice_period_prompt()\n",
        "    response = chain.invoke({\n",
        "        \"resume\": docs\n",
        "    })\n",
        "    return response\n",
        "\n",
        "def handle_user_input(user_question):\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model='text-embedding-3-small',\n",
        "        dimensions=1536,\n",
        "        base_url = endpoint,\n",
        "        api_key= github_token,)\n",
        "    new_db = FAISS.load_local(\"faiss_index\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
        "    docs = new_db.similarity_search(user_question)\n",
        "    chain = get_conversation_chain()\n",
        "    response = chain.invoke({\n",
        "        \"resume\": docs,\n",
        "        \"question\": user_question\n",
        "    })\n",
        "    return response\n",
        "\n",
        "def build_resume_df():\n",
        "    df = pd.DataFrame(columns=['Rank', 'Resume', 'Score', 'Content'])\n",
        "    return df\n",
        "\n",
        "st.set_page_config(\"Pre-Screening Intelligence: TalAI\", layout=\"wide\")\n",
        "\n",
        "left_col, middle_col, right_col = st.columns([1, 1, 1])\n",
        "\n",
        "# Initialize modal\n",
        "resume_modal = Modal(\"📂 Resume Tools\", key=\"resume-tools-modal\", max_width=\"700px\")\n",
        "\n",
        "\n",
        "section_list = [\n",
        "    'Work Experience',\n",
        "    'Education',\n",
        "    'Skills',\n",
        "    'Academic Projects',\n",
        "    'Certification, Award & Recognition',\n",
        "    'Career Objectives or Summary',\n",
        "    'Personal Details & Hobbies'\n",
        "]\n",
        "\n",
        "if \"resume_df\" not in st.session_state:\n",
        "    st.session_state.resume_df = build_resume_df()\n",
        "df = st.session_state.resume_df\n",
        "\n",
        "with left_col:\n",
        "    # Upload button triggers modal\n",
        "    if st.button(\"📤 Upload Resumes\"):\n",
        "        resume_modal.open()\n",
        "\n",
        "    # Modal content\n",
        "    # Modal content\n",
        "    if resume_modal.is_open():\n",
        "        with resume_modal.container():\n",
        "            st.markdown(\"\"\"\n",
        "            <div style=\"display: flex; flex-direction: column; align-items: center; text-align: center;\">\n",
        "            <img src=\"https://raw.githubusercontent.com/surabhi13gupta/LangChains/main/TalAI.png\" width=\"60\" style=\"border-radius: 50%; margin-bottom: 1rem;\">\n",
        "            <h4 style=\"margin-bottom: 0;\">TalAI ResumeTools</h4>\n",
        "            <p style=\"margin-top: 0;\">Upload your resume to extract insights, skills, and suggestions.</p>\n",
        "            </div>\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "            uploaded_files = st.file_uploader(\n",
        "                \"Upload Resumes (PDF, DOCX, ZIP allowed)\",\n",
        "                type=[\"pdf\", \"docx\", \"zip\"],\n",
        "                accept_multiple_files=True\n",
        "            )\n",
        "\n",
        "            stream = st.selectbox(\n",
        "                \"Select your Domain:\",\n",
        "                [\"Information Technology\", \"Finance\", \"Human Resources\", \"Sales\", \"Legal/Advocate\", \"Engineering\"]\n",
        "            )\n",
        "\n",
        "            if st.button(\"Submit and Process\") and uploaded_files:\n",
        "                with st.spinner(\"Processing...\"):\n",
        "                    rank = 1\n",
        "                    for uploaded_file in uploaded_files:\n",
        "                        all_texts = []\n",
        "                        ext = uploaded_file.name.rsplit('.', 1)[-1].lower()\n",
        "                        if ext == \"pdf\":\n",
        "                            modified_pdf = redact_personal_information(uploaded_file)\n",
        "                            all_texts.append(extract_text_from_pdf(modified_pdf))\n",
        "                        elif ext == \"docx\":\n",
        "                            all_texts.append(extract_text_from_docx(uploaded_file))\n",
        "                        elif ext == \"zip\":\n",
        "                            zip_bytes = BytesIO(uploaded_file.read())\n",
        "                            all_texts.extend(extract_from_zip(zip_bytes))\n",
        "                        # df.loc[len(df)] = [rank, uploaded_file.name, 10, all_texts]\n",
        "                        st.session_state.resume_df.loc[len(st.session_state.resume_df)] = [rank, uploaded_file.name, 10, all_texts]\n",
        "                        rank += 1\n",
        "\n",
        "                    # combined_text = \"\\n\\n\".join(all_texts)\n",
        "                    # text_chunks = get_text_chunks(combined_text)\n",
        "                    # get_vector_store(text_chunks)\n",
        "\n",
        "                    st.success(\"Resumes uploaded, processed and indexed!\")\n",
        "                    st.session_state.chat_history = []\n",
        "                    st.button(\"✅ Close\", on_click=lambda: resume_modal.close())\n",
        "            else:\n",
        "                st.info(\"Awaiting file upload...\")\n",
        "\n",
        "    if st.button(\"🗑️ Clear Resumes\"):\n",
        "        st.session_state.resume_df = build_resume_df()\n",
        "    st.header(\"📊 Top 5 Resumes\")\n",
        "\n",
        "    # Job description input\n",
        "    job_description = st.text_area(\n",
        "        \"📝 Job Description\",\n",
        "        height=200,\n",
        "        placeholder=\"Paste the job role or requirements here...\"\n",
        "    )\n",
        "\n",
        "    for section in section_list:\n",
        "        with st.expander(f\"📂 {section}\"):\n",
        "            st.text_area(f\"✏️ Edit or review: {section}\", height=150, placeholder=f\"Enter details for {section}...\")\n",
        "\n",
        "    # Submit button\n",
        "    if st.button(\"🚀 Match Resumes\") and job_description:\n",
        "        with st.spinner(\"Matching resumes...\"):\n",
        "            # Placeholder for vector search logic\n",
        "            top_matches = [\n",
        "                \"**1. Jane Doe** — ML Engineer, 5 yrs exp, NLP-heavy projects\",\n",
        "                \"**2. Ravi Kumar** — Time Series Specialist, fintech background\",\n",
        "                \"**3. Aisha Rahman** — GenAI pipeline builder, LangChain expert\",\n",
        "                \"**4. Leo Zhang** — Resume parsing wizard, UX-focused\",\n",
        "                \"**5. Sara Ali** — Dashboard designer, Streamlit + LLM integration\"\n",
        "            ]\n",
        "\n",
        "            # Display results\n",
        "            st.markdown(\"### 🏆 Top Resume Matches\")\n",
        "            for match in top_matches:\n",
        "                st.markdown(f\"- {match}\")\n",
        "\n",
        "with middle_col:\n",
        "    st.subheader(\"🔍 Summarization Dashboard\")\n",
        "\n",
        "    # First Section: Table Placeholder\n",
        "    st.markdown(\"### 📋 Resume Rank Table\")\n",
        "    st.write(\"Table content goes here...\")\n",
        "    # Track selected rows\n",
        "    selected_rows = []\n",
        "\n",
        "    # Display each row with a checkbox\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        cols = st.columns([1, 1, 1, 1])  # Adjust column widths\n",
        "        cols[0].write(df.loc[i, \"Rank\"])\n",
        "        cols[1].write(df.loc[i, \"Resume\"])\n",
        "        cols[2].write(df.loc[i, \"Score\"])\n",
        "        selected = cols[3].checkbox(\"Review\", key=f\"select_{i}\")\n",
        "        if selected:\n",
        "            selected_rows.append(df.loc[i])\n",
        "\n",
        "    # Show selected rows\n",
        "    if selected_rows:\n",
        "        st.write(\"### You selected:\")\n",
        "        st.dataframe(pd.DataFrame(selected_rows))\n",
        "\n",
        "    # st.dataframe({\n",
        "    #     \"Item\": [\"A\", \"B\", \"C\"],\n",
        "    #     \"Value\": [10, 20, 30]\n",
        "    # })\n",
        "\n",
        "    # Divider (optional)\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # Second Section: Text Summarization\n",
        "    st.markdown(\"### 📝 Text Summary\")\n",
        "    st.text_area(\"Summary\", \"This is your text summary...\", height=200)\n",
        "\n",
        "    # # Divider (optional)\n",
        "    # st.markdown(\"---\")\n",
        "\n",
        "    # # Third Section: Video Summarization\n",
        "    # st.markdown(\"### 🎥 Video Summary\")\n",
        "    # st.button(\"Summarize Video\")\n",
        "    # st.write(\"Video summary will appear here.\")\n",
        "\n",
        "with right_col:\n",
        "    st.markdown(\"### 🎥 Video Summary\")\n",
        "    st.button(\"Summarize Video\")\n",
        "    st.write(\"Video summary will appear here.\")\n",
        "\n",
        "    # Divider (optional)\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # Chat popover inside right column\n",
        "    with st.popover(\"💬 Chat with TalAI\", icon=\":material/chat:\", width=\"content\"):\n",
        "        st.header(\"💬 Chat with TalAI\")\n",
        "\n",
        "        if \"chat_history\" not in st.session_state:\n",
        "            st.session_state.chat_history = []\n",
        "\n",
        "        for msg in st.session_state.chat_history:\n",
        "            role = \"user\" if msg[\"role\"] == \"user\" else \"assistant\"\n",
        "            st.chat_message(role).write(msg[\"content\"])\n",
        "\n",
        "        user_input = st.chat_input(\"Type your message...\")\n",
        "        if user_input:\n",
        "            st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "            st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": f\"Echo: {user_input}\"})\n",
        "\n",
        "    # with st.popover(\"💬 Chat with TalAI\", icon=\":material/chat:\", width=\"content\"):\n",
        "    #     st.header(\"💬 Chat with TalAI\")\n",
        "\n",
        "    #     # Initialize chat history\n",
        "    #     if \"chat_history\" not in st.session_state:\n",
        "    #         st.session_state.chat_history = []\n",
        "\n",
        "    #     # Display chat messages\n",
        "    #     for msg in st.session_state.chat_history:\n",
        "    #         role = \"user\" if msg[\"role\"] == \"user\" else \"assistant\"\n",
        "    #         st.chat_message(role).write(msg[\"content\"])\n",
        "\n",
        "    #     # Input container for new messages\n",
        "    #     user_input = st.chat_input(\"Type your message...\")\n",
        "    #     if user_input:\n",
        "    #         st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "    #         st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": f\"Echo: {user_input}\"})\n",
        "# with right_col:\n",
        "#     st.header(\"💬 Chat with TalAI\")\n",
        "#     if \"chat_history\" not in st.session_state:\n",
        "#         st.session_state.chat_history = []\n",
        "\n",
        "#     for msg in st.session_state.chat_history:\n",
        "#         role = \"user\" if msg[\"role\"] == \"user\" else \"assistant\"\n",
        "#         st.chat_message(role).write(msg[\"content\"])\n",
        "\n",
        "#     chat_container = st.container()\n",
        "\n",
        "    # # Handle input first\n",
        "    # first_question = \"Candidate Name, total years of experience and recent job organisation\"\n",
        "    # st.session_state.chat_history.append({\n",
        "    #         \"role\": \"user\",\n",
        "    #         \"content\": first_question\n",
        "    #     })\n",
        "    # recent_job = handle_user_input(first_question)\n",
        "    # st.session_state.chat_history.append({\n",
        "    #     \"role\": \"assistant\",\n",
        "    #     \"content\": recent_job\n",
        "    # })\n",
        "\n",
        "    # st.session_state.chat_history.append({\n",
        "    #         \"role\": \"user\",\n",
        "    #         \"content\": \"Notice Period information for recent company\"\n",
        "    #     })\n",
        "    # notice_response = handle_notice_period()\n",
        "    # st.session_state.chat_history.append({\n",
        "    #     \"role\": \"assistant\",\n",
        "    #     \"content\": notice_response\n",
        "    # })\n",
        "\n",
        "    # user_question = st.chat_input(\"Ask TalAI about the selected resume...\")\n",
        "    # if user_question:\n",
        "    #     st.session_state.chat_history.append({\n",
        "    #         \"role\": \"user\",\n",
        "    #         \"content\": user_question\n",
        "    #     })\n",
        "    #     response = handle_user_input(user_question)\n",
        "    #     st.session_state.chat_history.append({\n",
        "    #         \"role\": \"assistant\",\n",
        "    #         \"content\": response\n",
        "    #     })\n",
        "\n",
        "    # # Render all messages inside chat_container\n",
        "    # with chat_container:\n",
        "    #     for msg in (st.session_state.chat_history):  # Latest at top\n",
        "    #         role = \"user\" if msg[\"role\"] == \"user\" else \"assistant\"\n",
        "    #         prefix = \"You: \" if role == \"user\" else \"TalAI: \"\n",
        "    #         st.chat_message(role).write(prefix + msg[\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rPY3Nlx9q4W",
        "outputId": "f5191107-fe20-4d3c-a9f0-94ddb0ef380a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit URL: NgrokTunnel: \"https://1244d3640131.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "public_url = ngrok.connect(addr=8501)\n",
        "\n",
        "print(f\"Streamlit URL: {public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olrmCFD89wyA",
        "outputId": "f9d516e1-1338-46c9-c1fe-0045cf43dc57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.73.172.30:8501\u001b[0m\n",
            "\u001b[0m\n",
            "Enter\n",
            "Enter\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!streamlit run main.py&"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8/vj9vDorTCoW12aXa+Cy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}